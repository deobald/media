
NOTES:
  - references are optional (but I recommend them)
  - headers are optional
  - should we discuss handover/KT? (see TODO)

# Case Study: Insider.in

PayTM Insider is India's largest online ticketing and events company. Insider approached nilenso in early 2018 with a concept for an entirely new business model. Millions of fans were already coming to Insider's live events. Insider wanted to bring those events online. Could nilenso help them host online events --- with software hosting all those millions of fans _at once_?


## The Problem

It wasn't a foregone conclusion that we could build such a system. We certainly had never built one like it before. Even software like Twitch.tv, dedicated to streaming video to simultaneous users, lives in the realm of "millions of simultaneous users". Insider's live events, such as NH7 Weekender, were known to draw crowds large enough to bring commercial telecommunications infrastructure to its knees. Building a platform to host even larger online events? We were intrigued.

The project began with a disarmingly simple problem statement: "Stream interactive video to one million concurrent users." Outside of a real emergency, we would never normally work weekends, but we had worked with the team from Insider before and we were just as excited as they were about solving this puzzle. We spent that first weekend split into teams, battling a GoLang prototype against an Elixir prototype. (By which, we really mean Erlang... by which, we really mean BEAM/ERTS/OTP, Erlang's Virtual Machine (VM)/Runtime/Core Libraries. More on that later.) Our usual weapon of choice, Clojure, wasn't a good fit for a system that demanded one million simultaneous connections because the JVM's scaling models were simply too heavy. GoLang has some wonderful concurrency primitives, but they proved no match for Erlang's ultralightweight processes, OTP (which, for purely historical reasons, stands for "Open Telecom Platform") tooling, and in-your-face distributed systems approach. By the end of the weekend, Elixir had won the brawl and the project had a name: _ngage_.


## The Solution

Building on the Erlang runtime meant distributed systems were baked into every component and, while the team was very experienced with distributed systems [1] and the Patterns of Enterprise Application Architecture [2], our past toolkits rarely _enforced_ message-passing distributed architectures; it was a choice we made. However, BEAM (and therefore, Elixir) declare, existentially, that message-passing and process boundaries (and their inverse: process size), are the primary considerations of system design. To build a functional system on BEAM at all, you must build a distributed system. Over the first few weeks of the project, the team deeply embedded themselves in the Erlang runtime paradigm --- learning, experimenting, and exploring along the way. The team had chosen Elixir precisely for its runtime and when it was time to load test the system, the distributed-system-up-front model had already proven itself.

Load testing is an exercise delineated by orders of magnitude. It becomes a non-trivial task when simulating concurrent users ordering in the hundreds of thousands. The team nicknamed these thousands of gossamer androids "The Mob"... built to swarm _ngage_ with parallel but uncoordinated activity in hopes of breaking the system in the safety of a sandbox. The Mob[ile] Simulator (_mobsim_ was a double entendre, since the anticipated million users were to connect exclusively through mobile devices) would continuously test _ngage_, looking for weaknesses in its distributed design, in turn helping the team carve out the all-important process boundaries. Neena would eventually transform the lessons from these iterations into his talk, "After The Crash", discussing systems design on the BEAM VM. [3] In his words, "many problems are simply avoided altogether because Erlang puts the complexity of scaling distributed systems front-and-centre, even when you run the system on a laptop."

Over the project's 18-month journey, starting with that first furious weekend and
ending with the project handover, there were more landmark achievements than the
million-concurrent-users requirement alone. A digital event isn't really an "event"
if it's not interactive. Insider hadn't hired us to build YouTube. The first
incarnation was a massive quiz show, which meant that the hundreds of thousands of
participants needed to see the quiz questions at the _exact same time_. Initially,
even the synchronization of questions to the video feed created difficulties. Video
needed to be transcoded into a variety of formats for different users and, even after
a number of transcoding optimizations, video delivered to mobile devices was
consistently delayed by almost 10 full seconds. Synchronizing systems by timestamp
was nothing new to the nilenso team but it was a new exercise to stamp metadata into
streaming video. This self-identified the sync time and compensated for server-side
transcoding delays in the mobile clients, ensuring questions always appeared to users
at exactly the right time.

The Insider team running an event of course needed plenty of tools. The massive
multi-user events could be meaningfully compared to Massively Multiplayer Online Games (MMOs) as
each event was, in many ways, an MMO. But Insider events had the additional complexity
of central coordination. If you are familiar with _Dungeons & Dragons_, you can think
of the events team at Insider working together as a collective _Dungeon Master_. In a quiz
show, for example, the presenter is the face of the show. But there are many other
folks behind the scenes, managing teleprompters, publishing questions and answers to
the massive live audience, and coordinating transitions from one question to the
next. To coordinate this entire effort, nilenso built Insider a dashboard to control
_ngage_. With this tool in hand, Insider wasn't just streaming mammoth amounts of
videos and other data to millions of users simultaneously --- they were doing so in a carefully
coordinated fashion.

Other project landmarks were not of a technical nature at all. The Insider team was
incredibly busy with myriad projects at their headquarters in Bombay. Although both
teams were operating on India Standard Time, it was often difficult for anyone in
Bombay to make time for conversations... and a much bigger struggle to find time for
prioritization. The counterintuitive solution was one many of nilenso's clients have
chosen in the past: let nilenso drive the project. Insider knows events; nilenso
knows software. The nilenso team would construct technical priorities and pursue
them, reviewing the pending work with Insider whenever possible.

On other occasions, the technical and the non-technical would intersect. When it
became clear that Insider did not have the time to build video transcoding and streaming into
_ngage_, the team agreed that nilenso would build that as well. After
evaluating OBS (Open Broadcaster Software) with a custom plugin, the team ultimately
settled on a combination of mimoLive and Wowza Cloud. Although more expensive, this
solution allowed Insider to focus on building its massive, interactive events
platform rather than reinventing the media streaming wheel.

After months of research, exploration, and iterative development between the nilenso team, Insider's team in Bombay, and the mobile app team from Uncommon (now Able) in Hyderabad, it was time to take _ngage_ out of its sandbox. Battle-hardened after the simulations provided by _mobsim_, it would finally see real phones across India connect to it. All three teams, spread across Bombay, Bangalore, and Hyderabad, would connect and listen to nilenso's tech lead shout "question!" while sending out the synchronized quiz question for everyone to respond. In each office, an echo of "que-ques-qu-question-tion-ion-n!" could be heard tumbling out of all the phones in the room. After everyone was satisfied with the behaviour of the system in the contrived scenario, Insider opened _ngage_ up to the real world with its initial public betas.

Software projects of any complexity rarely make their maiden voyage without a hitch
and the early betas were no exception. Beta 1 went smoothly from a technical
standpoint, but the all-new format hadn't been marketed heavily enough and the first
quiz show didn't draw a big crowd. Insider was determined to see a big turnout for
Beta 2 --- advertisements went up, notices went out. And this time, a real crowd
showed up. In anticipation, the team adjusted the settings of _ngage_, increasing the
number of live processes, to correspond to the anticipated load. Unfortunately, this
revealed a bug that _mobsim_ hadn't uncovered yet. Satellite ("worker") Erlang
processes are _designed_ to safely handle crashes (hence the title of neena's talk:
"After The Crash") but when too many crashes occur, the supervisor process in charge
of restarting the workers can _crash itself_. If these failures cascade, the entire
Erlang Supervision Tree [4] can collapse. During the massive Beta 2, this was
precisely what happened.

Humbled, the team packed their bags and headed to Bombay. When the going gets tough,
there's nothing quite like colocation in a single office. [5] With everyone under one roof and communication between all three teams flowing effortlessly, _ngage_ learned to handle millions of concurrent users and _mobsim_ became the platform on which it learned. Within weeks, the initial question-answer "quiz" format was exchanged for a more flexible vote-based "poll" format. Insider successfully ran new betas including a live comedy compeition, scored by the audience, and a live choose-your-own-adventure horror story, guided by audience choices. The Insider project, which was to become known as _Insider Gameshows_, was a success.


## Outcomes

No one knows whether a software project will be a success. Product competition, faulty business models, design and architecture mistakes... anything can lead to a project's downfall and, more often than not, this is the eventual conclusion. The success of _Insider Gameshows_ meant the product had a market fit, the mobile app worked flawlessly for end users, and _ngage_ stood up to the massive loads it needed to carry.

Ultimately, the new "poll" archetype gave Insider a very flexible format on which it could run endless online events. Elixir and the BEAM VM proved itself over and over. Some end users may have witnessed the flawed Beta 2 release, but the design characteristics innate to Erlang prevented countless other scalability problems. What about our friend _mobsim_? It's still in use today, acting as the load regression framework for every new release of _ngage_.

- TODO (?) transition to in-house team

The software industry frequently reaches for descriptions like "challenging work" and "world-class developers"... frequently enough that these words have become meaningless. There exists a dichotomy in the world of software consulting that we often fumble to elucidate. On one hand, some custom software lives in the world of the _bespoke_. A good tailor does not reinvent the craft of tailoring every time she cuts a new suit. Similarly, the three million apps on the stores of iOS and Android largely don't represent revolutions in Computer Science. Humanity isn't the proud custodian of billions of web apps because they each represent a brainchild of software engineering's avant-garde. Software in the 21st Century is wildly democratic, which is wonderful because each individual developer doesn't have the energy to rediscover the very definition of a mobile app every time she writes one. On the other hand, however, there does exist software which is truly experimental and genuinely difficult to produce. This is it.


## References

[1] Talks by nilenso employees - https://youtube.com/c/nilenso
[2] "Patterns of Enterprise Application Architecture" - https://dl.acm.org/doi/book/10.5555/579257
[3] "After The Crash" - https://www.youtube.com/watch?v=9nbPZOHBsK4
[4] "Erlang Supervision Principles" - http://erlang.org/documentation/doc-4.9.1/doc/design_principles/sup_princ.html
[5] "Lean Software Development" - https://www.amazon.com/exec/obidos/ASIN/0321150783/poppendieckco-20
